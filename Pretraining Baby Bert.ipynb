{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5932e8ad-ebcb-4ce1-ae7c-a3b60d21d7b0",
   "metadata": {},
   "source": [
    "### Importing The Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64bec614-65d1-4b2a-866b-5dd518d9ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchtext.vocab import Vocab,build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import IMDB\n",
    "import random\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82a84a-d9b0-4b91-99d9-5301b8801cd9",
   "metadata": {},
   "source": [
    "### Loading The CSV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2cc06b0-46be-4772-99b5-ef5d614b0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCSVDataset(Dataset):\n",
    "    def __init__(self,filename):\n",
    "        self.data=pd.read_csv(filename)\n",
    "        self.tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        try:\n",
    "            \n",
    "            bert_input = torch.tensor(json.loads(row['BERT Input']), dtype=torch.long)\n",
    "            bert_label = torch.tensor(json.loads(row['BERT Label']), dtype=torch.long)\n",
    "            segment_label = torch.tensor([int(x) for x in row['Segment Label'].split(',')], dtype=torch.long)\n",
    "            is_next = torch.tensor(row['Is Next'], dtype=torch.long)\n",
    "            original_text = row['Original Text']  # If you want to use it\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for row {idx}: {e}\")\n",
    "            print(\"BERT Input:\", row['BERT Input'])\n",
    "            print(\"BERT Label:\", row['BERT Label'])\n",
    "            # Handle the error, e.g., by skipping this row or using default values\n",
    "            return None  # or some default values\n",
    "\n",
    "            # Tokenizing the original text with BERT\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            original_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_input['input_ids'].squeeze()\n",
    "        attention_mask = encoded_input['attention_mask'].squeeze()\n",
    "\n",
    "        return(bert_input, bert_label, segment_label, is_next, input_ids, attention_mask, original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b81aac33-e55c-4b4c-9b2d-ac5aa6b6ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "def collate_batch(batch):\n",
    "\n",
    "   \n",
    "    bert_inputs_batch, bert_labels_batch, segment_labels_batch, is_nexts_batch,input_ids_batch,attention_mask_batch,original_text_battch = [], [], [], [],[],[],[]\n",
    "\n",
    "    for bert_input, bert_label, segment_label, is_next,input_ids,attention_mask,original_text in batch:\n",
    "        # Convert each sequence to a tensor and append to the respective list\n",
    "        bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
    "        bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
    "        segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
    "        is_nexts_batch.append(is_next)\n",
    "        input_ids_batch.append(input_ids)\n",
    "        attention_mask_batch.append(attention_mask)\n",
    "        original_text_battch.append(original_text)\n",
    "\n",
    "    # Pad the sequences in the batch\n",
    "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    bert_labels_final = pad_sequence(bert_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)\n",
    "\n",
    "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851da32f-5962-42f9-ad14-6b72c00e1f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataset_path = './bert_dataset/bert_train_data.csv'\n",
    "test_dataset_path = './bert_dataset/bert_test_data.csv'\n",
    "\n",
    "train_dataset = BERTCSVDataset(train_dataset_path)\n",
    "test_dataset = BERTCSVDataset(test_dataset_path)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_batch)\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d62c296-7b39-496c-ad54-ee6dea9e9de2",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c090a0b7-eba5-4970-8a6f-62f44e6feba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=10\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim):\n",
    "        super(TokenEmbedding,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_dim)\n",
    "        self.embed_dim=embed_dim\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "789251a6-fd72-4573-8cc3-551ebbb69c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c4d3daf-8b26-4829-a884-7103ffbb93a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,dropout=0.1,train=True):\n",
    "        super(BERTEmbedding,self).__init__()\n",
    "        self.token_embedding=TokenEmbedding(vocab_size,embed_dim)\n",
    "        self.positional_encoding=PositionalEncoding(embed_dim,dropout)\n",
    "        self.segment_embedding = nn.Embedding(3, embed_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    def forward(self,bert_inputs,segement_labels=False):\n",
    "        my_embeddings=self.token_embedding(bert_inputs)\n",
    "        if self.train:\n",
    "          x = self.dropout(my_embeddings + self.positional_encoding(my_embeddings) + self.segment_embedding(segment_labels))\n",
    "        else:\n",
    "          x = my_embeddings + self.positional_encoding(my_embeddings)\n",
    "          return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "469e5541-b7d4-4e09-a2ec-eb104658cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=147161\n",
    "batch=2\n",
    "count=0\n",
    "for batch in train_dataloader:\n",
    "    bert_input,bert_label,segement_label,is_next=[b for b in batch]\n",
    "    \n",
    "    count+=1\n",
    "    if count==5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c1f8c8-22e1-4f9f-80a1-bf588832ac10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1a7a36e-6ef0-48b8-a216-e4bce654cd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94f112d8-4af2-44ce-b0ba-e49cd65ae509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segement_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ea50992-af8b-43cb-85ff-2222936d77cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_next.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "253e7636-7186-44df-b074-41605ce1937f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4805fd3b-d46c-4fd3-b27b-a4b709473915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     1],\n",
       "        [    8,    16],\n",
       "        [    3,     3],\n",
       "        [   13,     9],\n",
       "        [    3,    96],\n",
       "        [93745,   541],\n",
       "        [   10,     3],\n",
       "        [   35,    12],\n",
       "        [  127, 38782],\n",
       "        [  125,     3],\n",
       "        [ 2643,    19],\n",
       "        [  134,   131],\n",
       "        [   39,    18],\n",
       "        [    2,    20],\n",
       "        [    0,     9],\n",
       "        [   98,   359],\n",
       "        [   17,     7],\n",
       "        [  287,   683],\n",
       "        [   12, 10579],\n",
       "        [    3,    43],\n",
       "        [   66,   348],\n",
       "        [ 2382,     6],\n",
       "        [   22,     2],\n",
       "        [    3,    14],\n",
       "        [ 1756,    12],\n",
       "        [ 1390,    19],\n",
       "        [    3,    47],\n",
       "        [   81,   619],\n",
       "        [    6,    11],\n",
       "        [    2,    77],\n",
       "        [    0,    18],\n",
       "        [    0,   348],\n",
       "        [    0, 10675],\n",
       "        [    0,   562],\n",
       "        [    0,   221],\n",
       "        [    0,     3],\n",
       "        [    0,    23],\n",
       "        [    0,   153],\n",
       "        [    0,   208],\n",
       "        [    0,   143],\n",
       "        [    0,   502],\n",
       "        [    0,     6],\n",
       "        [    0,     2],\n",
       "        [    0,     0],\n",
       "        [    0,     0],\n",
       "        [    0,     0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6213f95-2ca2-4c73-92ab-7bc36c4dd8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [2, 1],\n",
       "        [2, 1],\n",
       "        [2, 1],\n",
       "        [2, 1],\n",
       "        [2, 1],\n",
       "        [2, 1],\n",
       "        [2, 1],\n",
       "        [2, 1],\n",
       "        [2, 2],\n",
       "        [2, 2],\n",
       "        [2, 2],\n",
       "        [2, 2],\n",
       "        [2, 2],\n",
       "        [2, 2],\n",
       "        [2, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segement_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d8327ce-8484-44e7-8351-152832915839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of token embeddings: torch.Size([46, 2, 10])\n",
      "Token Embeddings for the 0th token of the first sample: tensor([-5.2643,  2.1061,  0.6387,  3.3791, -2.7117,  4.6467,  0.3394,  3.1973,\n",
      "        -4.3253, -0.3849], grad_fn=<SliceBackward0>)\n",
      "Token Embeddings for the 1th token of the first sample: tensor([-1.6590,  1.3288,  6.5083, -6.4037, -6.3066, -1.2353,  0.4244, -0.3644,\n",
      "         2.0359, -0.9853], grad_fn=<SliceBackward0>)\n",
      "Token Embeddings for the 2th token of the first sample: tensor([-0.5932,  1.2142, -3.1347, -1.2464,  0.6761, -3.4110,  1.2599, -1.0130,\n",
      "         2.8166,  3.5121], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the TokenEmbedding \n",
    "token_embedding = TokenEmbedding(VOCAB_SIZE, embed_dim=EMBEDDING_DIM )\n",
    "\n",
    "# Get the token embeddings for a sample input\n",
    "t_embeddings = token_embedding(bert_input)\n",
    "#Each token is transformed into a tensor of size emb_size\n",
    "print(f\"Dimensions of token embeddings: {t_embeddings.size()}\") # Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
    "#Check the embedded vectors for first 3 tokens of the first sample in the batch\n",
    "# you get embeddings[i,0,:] where i refers to the i'th token of the first sample in the batch (b=0)\n",
    "for i in range(3):\n",
    "    print(f\"Token Embeddings for the {i}th token of the first sample: {t_embeddings[i,0,:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caadec7-ef16-424a-ad27-f1b9b6750c5c",
   "metadata": {},
   "source": [
    "### Bert Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "516282b7-eeda-4993-a0d5-6a9bc81dc243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,num_layers,n_head,dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "        self.embedding=BERTEmbedding(vocab_size,embed_dim)\n",
    "        encoder_layer=nn.TransformerEncoderLayer(d_model=embed_dim,nhead=n_head,dropout=dropout)\n",
    "        self.encoder=nn.TransformerEncoder(encoder_layer,num_layers=num_layers)\n",
    "        #Next Sentence Prediction\n",
    "        self.NextSentencePrediction=nn.Linear(embed_dim,2)\n",
    "        #Masked Word Prediction\n",
    "        self.MaskedPrediction=nn.Linear(embed_dim,vocab_size)\n",
    "    def forward(self,bert_input,segement_label):\n",
    "        padding_mask=(bert_inputs==PAD_IDX).transpose(0,1)\n",
    "        x=self.embedding(bert_input,segement_label)\n",
    "        values_after_encoding=self.encoder(x,src_key_padding_mask=padding_mask)\n",
    "        next_sentence=self.NextSentencePrediction(values_after_encoding[0,:])\n",
    "        masked_language=self.MaskedPrediction(values_after_encoding)\n",
    "        return next_sentence,masked_language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f10ab092-7b9e-4088-9a5a-08f9ee1a9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "vocab_size = 147161\n",
    "d_model = EMBEDDING_DIM  \n",
    "n_layers = 2\n",
    "initial_heads = 12\n",
    "initial_heads = 2\n",
    "heads = initial_heads - d_model % initial_heads\n",
    "dropout = 0.1  \n",
    "model = BERT(vocab_size, d_model, n_layers, heads, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770eb38-66f7-4cb3-9cf6-6cc8d3498aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
